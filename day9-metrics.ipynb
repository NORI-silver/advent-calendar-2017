{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## day9 評価とモデル選択"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "# 学習器\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# 評価指標\n",
    "from sklearn import metrics\n",
    "# モデル選択\n",
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2クラス問題\n",
    "breast_cancerを使ってみる"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "breast_cancer = load_breast_cancer()\n",
    "clf = LogisticRegression()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(breast_cancer.data, breast_cancer.target,train_size=0.8)\n",
    "clf.fit(X_train,y_train)\n",
    "# 結果を返す\n",
    "pred_y = clf.predict(X_test)\n",
    "# スコアを出すための関数。\n",
    "# ref : [decision_functionとの違いは？](https://stackoverflow.com/questions/36543137/whats-the-difference-between-predict-proba-and-decision-function-in-sklearn-py)\n",
    "pred_y_score = clf.predict_proba(X_test)[:,1]\n",
    "\n",
    "def report(y_test, pred_y, pred_y_score):\n",
    "    print(\"#################################\")\n",
    "    print(\"######CLASSIFICATION REPORT######\")\n",
    "    print(\"#################################\")\n",
    "    # Confusion Matrix\n",
    "    print(\"confusion matrix\\n\", metrics.confusion_matrix(y_test, pred_y))\n",
    "    print(\"accuracy\\n\", metrics.accuracy_score(y_test, pred_y))\n",
    "    print(\"classification report\\n\",metrics.classification_report(y_test,pred_y))\n",
    "    print(\"hamming loss\", metrics.hamming_loss(y_test, pred_y))\n",
    "    print(\"jaccard similarity\", metrics.jaccard_similarity_score(y_test,pred_y))\n",
    "#     print(\"log loss\", metrics.log_loss(y_test, pred_y_score)) \n",
    "    print(\"cohen kappa score\", metrics.cohen_kappa_score(y_test, pred_y))\n",
    "\n",
    "    # Compute ROC curve and ROC area for each class\n",
    "    fpr, tpr, _ = metrics.roc_curve(y_test, pred_y_score,pos_label=1)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "    print(\"auc area\", roc_auc)\n",
    "\n",
    "    #compute precision-recall curve\n",
    "    precision, recall, thresholds = metrics.precision_recall_curve(y_test, pred_y_score)\n",
    "    average_precision_score = metrics.average_precision_score(y_test,pred_y_score)\n",
    "    print(\"average precision score:\", average_precision_score)\n",
    "    # ROC曲線\n",
    "    plt.figure(figsize=(7,3.5))\n",
    "    plt.subplot(121)\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC curve: \\nAUC={:.2f}'.format(roc_auc))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "\n",
    "    # precision-recall curve\n",
    "    plt.subplot(122)\n",
    "    plt.plot()\n",
    "    plt.plot(recall, precision, color='b', alpha=0.2, label=\"RR curve\")\n",
    "    plt.fill_between(recall, precision, step='post', alpha=0.2,color='b')\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.title('Precision-Recall curve: \\nAP={0:0.2f}'.format(average_precision_score))\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "report(y_test, pred_y, pred_y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 実際に比べてみる\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_moons\n",
    "X, y = make_moons(n_samples=200, noise=0.2)\n",
    "# 学習器\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "# scaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "h = .02  # step size in the mesh\n",
    "names = [\"LogisticRegression\",\"Nearest Neighbors\", \"Linear SVM\", \"RBF SVM\", \"Gaussian Process\",\n",
    "         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n",
    "         \"Naive Bayes\", \"QDA\"]\n",
    "\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    KNeighborsClassifier(3),\n",
    "    SVC(kernel=\"linear\", C=0.025),\n",
    "    SVC(gamma=2, C=1),\n",
    "    GaussianProcessClassifier(1.0 * RBF(1.0)), \n",
    "    DecisionTreeClassifier(max_depth=5),\n",
    "    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n",
    "    MLPClassifier(alpha=1),\n",
    "    AdaBoostClassifier(),\n",
    "    GaussianNB(),\n",
    "    QuadraticDiscriminantAnalysis()    \n",
    "]\n",
    "X = StandardScaler().fit_transform(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.5, random_state=42)\n",
    "\n",
    "x_min, x_max = X[:, 0].min() - .5, X[:, 0].max() + .5\n",
    "y_min, y_max = X[:, 1].min() - .5, X[:, 1].max() + .5\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "# just plot the dataset first\n",
    "cm = plt.cm.RdBu\n",
    "row = 2\n",
    "cm_bright = ListedColormap(['#FF0000', '#0000FF'])\n",
    "figure = plt.figure(figsize=(18, 6))\n",
    "ax = plt.subplot(row, np.ceil(len(classifiers)/row), 1)\n",
    "ax.set_title(\"original\")\n",
    "# Plot also the training points\n",
    "ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors='gray')\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, alpha=0.6)\n",
    "# 各学習器プロットする/結果を保存する\n",
    "cls_result = {}\n",
    "for i,(name, clf) in enumerate(zip(names, classifiers)):\n",
    "    ax = plt.subplot(row, np.ceil(len(classifiers)/row), i+2)\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Plot the decision boundary. For that, we will assign a color to each\n",
    "    # point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        Z = clf.decision_function(np.c_[xx.ravel(), yy.ravel()])\n",
    "    else:\n",
    "        Z = clf.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:, 1]\n",
    "    \n",
    "    # Put the result into a color plot\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    ax.contourf(xx, yy, Z, cmap=cm, alpha=.8)\n",
    "    # scattering original data\n",
    "    ax.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap=cm_bright, edgecolors='k')\n",
    "    ax.scatter(X_test[:, 0], X_test[:, 1], c=y_test, cmap=cm_bright, edgecolors='k', alpha=0.6)\n",
    "\n",
    "    ax.set_xlim(xx.min(), xx.max())\n",
    "    ax.set_ylim(yy.min(), yy.max())\n",
    "    ax.set_xticks(())\n",
    "    ax.set_yticks(())\n",
    "    ax.set_title(name)\n",
    "    # テスト用\n",
    "    pred_y = clf.predict(X_test)\n",
    "    if hasattr(clf, \"decision_function\"):\n",
    "        pred_y_score = clf.decision_function(X_test)\n",
    "    else:\n",
    "        pred_y_score = clf.predict_proba(X_test)[:,1]\n",
    "    cls_result[name] = {}\n",
    "    cls_result[name][\"pred_y\"] = pred_y\n",
    "    cls_result[name][\"y_test\"] = y_test\n",
    "    cls_result[name][\"pred_y_score\"] = pred_y_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    print(name)\n",
    "    pred_y = cls_result[name][\"pred_y\"]\n",
    "    y_test = cls_result[name][\"y_test\"]\n",
    "    pred_y_score = cls_result[name][\"pred_y_score\"]\n",
    "    report( y_test,pred_y,pred_y_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression\n",
    "- variance score\n",
    "- mean absolute error (MAE)\n",
    "- mean squared error (MSE)\n",
    "- median absolute error\n",
    "- R2 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import model_selection, metrics\n",
    "import numpy as np\n",
    "boston = load_boston()\n",
    "reg = LinearRegression()\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(boston.data, boston.target,train_size=0.8)\n",
    "reg.fit(X_train,y_train)\n",
    "pred_y = reg.predict(X_test)\n",
    "print(\"Variance score\", metrics.explained_variance_score(y_test, pred_y))\n",
    "print(\"MAE : Mean absolute error\", metrics.mean_absolute_error(y_test, pred_y))\n",
    "print(\"MSE : Mean squared error\", metrics.mean_squared_error(y_test, pred_y))\n",
    "print(\"RMSE: Root mean squared error\", np.sqrt(metrics.mean_squared_error(y_test, pred_y)))\n",
    "print(\"R2 score\", metrics.r2_score(y_test, pred_y))\n",
    "# print(\"Median sbsolute error\", metrics.median_absolute_error(y_test, pred_y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "data, target = make_moons(noise=0.05,n_samples=1000)\n",
    "base_colors = [\"r\", \"g\"]\n",
    "colors = [base_colors[i] for i in target]\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.scatter(data[:,0],data[:,1], color=colors, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html\n",
    "from sklearn import cluster, datasets, mixture\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "params = {'quantile': .3,\n",
    "                'eps': .3,\n",
    "                'damping': .9,\n",
    "                'preference': -200,\n",
    "                'n_neighbors': 10,\n",
    "                'n_clusters': 2}\n",
    "bandwidth = cluster.estimate_bandwidth(data, quantile=params['quantile'])\n",
    "kmeans = cluster.KMeans(n_clusters=params[\"n_clusters\"])\n",
    "ms = cluster.MeanShift(bandwidth=bandwidth, bin_seeding=True)\n",
    "two_means = cluster.MiniBatchKMeans(n_clusters=params['n_clusters'])\n",
    "ward = cluster.AgglomerativeClustering(n_clusters=params['n_clusters'], linkage='ward')\n",
    "spectral = cluster.SpectralClustering(n_clusters=params['n_clusters'], eigen_solver='arpack',affinity=\"nearest_neighbors\")\n",
    "dbscan = cluster.DBSCAN(eps=params['eps'])\n",
    "affinity_propagation = cluster.AffinityPropagation(damping=params['damping'], preference=params['preference'])\n",
    "average_linkage = cluster.AgglomerativeClustering(linkage=\"average\", affinity=\"cityblock\", n_clusters=params['n_clusters'])\n",
    "birch = cluster.Birch(n_clusters=params['n_clusters'])\n",
    "gmm = mixture.GaussianMixture(n_components=params['n_clusters'], covariance_type='full')\n",
    "\n",
    "clustering_algorithms = [\n",
    "    (\"KMeans\", kmeans),\n",
    "    ('MiniBatchKMeans', two_means),\n",
    "    ('AffinityPropagation', affinity_propagation),\n",
    "    ('MeanShift', ms),\n",
    "    ('SpectralClustering', spectral),\n",
    "    ('Ward', ward),\n",
    "    ('AgglomerativeClustering', average_linkage),\n",
    "    ('DBSCAN', dbscan),\n",
    "    ('Birch', birch),\n",
    "#     ('GaussianMixture', gmm)\n",
    "]\n",
    "from sklearn.cluster import KMeans\n",
    "row = 3\n",
    "plt.figure(figsize=(9,9))\n",
    "for i,(name, algo) in enumerate(clustering_algorithms):\n",
    "    print(name,algo)\n",
    "    pred_y = algo.fit_predict(data)\n",
    "    c = [base_colors[i] for i in pred_y]\n",
    "    plt.subplot(row, len(clustering_algorithms)/row,i+1)\n",
    "    plt.scatter(data[:,0],data[:,1], color=c, alpha=0.5)\n",
    "    plt.title(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
