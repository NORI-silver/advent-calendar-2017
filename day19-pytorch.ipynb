{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch\n",
    "-    A replacement for numpy to use the power of GPUs\n",
    "-    a deep learning research platform that provides maximum flexibility and speed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.Tensor(5,3) #ゼロ行列\n",
    "y = torch.rand(5,3) # ランダム行列\n",
    "# 演算\n",
    "print(x.add_(y)) #アンダースコアをつける\n",
    "print(torch.add(x,y))\n",
    "print(x+y)\n",
    "# numpy likeに扱える\n",
    "print(y[:,1],y.mean(), y.std()) \n",
    "print(torch.ones(10))\n",
    "print(torch.ones(10)+1)\n",
    "# numpyからの変換も可能\n",
    "import numpy as np\n",
    "a = np.arange(4).reshape(2,2)\n",
    "x = torch.from_numpy(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **define-by-run framework**\n",
    "Chainerの血を受け継ぐPyTorchの大きな特徴。\n",
    "\n",
    ">  It is a define-by-run framework, which means that your backprop is defined by how your code is run, and that every single iteration can be different.\n",
    "\n",
    "- ネットワークを**動的に**変更できる。\n",
    "    - 柔軟な設計が可能。\n",
    "    - 1イテレーションごとに設計を変えられる\n",
    "    - [参考:ChainerのDefine by Runとは？](http://s0sem0y.hatenablog.com/entry/2017/01/14/060758)\n",
    "\n",
    "## Autograd\n",
    "- `torch.autograd`\n",
    "`Function`と`Variables`で定義すると、**演算結果が保存される**ので、\n",
    "そのままさかのぼった微分が可能。\n",
    "\n",
    "下記の式は、\n",
    "\n",
    "- $x = [[1,2],[3,4]]$\n",
    "- $y = x^{2} +2$\n",
    "- $z = y^{2} = (x^{2}+2)^{2}$\n",
    "\n",
    "$out = \\frac{1}{4}\\sum_{i} z_{i}z_{i} = (x^{2}+2)^{2}$\n",
    "より、\n",
    "$\\frac{\\partial out}{\\partial x_{i}}=x_{i}(x_{i}^{2}+2)$\n",
    "\n",
    "その結果を返す\n",
    " \n",
    "[more](http://pytorch.org/docs/master/autograd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "x = Variable(torch.FloatTensor([[1,2],[3,4]]), requires_grad=True)\n",
    "y = x.float()**2 + 2\n",
    "print(y.grad_fn)\n",
    "z = y**2\n",
    "out = z.mean()\n",
    "out.backward() # \n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NeuralNetの構築\n",
    "\n",
    "### 学習の手順\n",
    "\n",
    "- 学習ができるニューラルネットを定義する。\n",
    "- データセットを繰り返し投げられるようにする。\n",
    "- インプットに応じて、処理(計算)が走るようにする  \n",
    "- 誤差(損失)を計算する\n",
    "- back propagationを実施する\n",
    "- optimizerによってパラメータを更新する\n",
    "\n",
    "この中で**データセットを繰り返し投げる以外**の要素を説明していく。\n",
    "### ライブラリ構成\n",
    "PyTorchのよく使うパッケージ\n",
    "\n",
    "|パッケージ|説明|\n",
    "|---|---|\n",
    "|`torch`|\tNumPyのような配列(テンソル)ライブラリ。GPUも簡単に使える|\n",
    "|`torch.autograd`|自動微分ライブラリ。Torchで定義されたすべての関数に対して微分を可能にしている|\n",
    "|`torch.nn`|ネットワークを構成する要素|\n",
    "|`torch.optim`|パラメータ更新時に使う最適化パッケージ|\n",
    "|`torch.utils`|DataLoaderなど、その他のユーティリティ関数|\n",
    "\n",
    "\n",
    "### torch.nn \n",
    "ネットワークを構成する要素関連はここに入っている\n",
    "- `Conv1d`,`Conv2d`,`MaxPool1d`, `AvePool1d`, `AdaptiveAvgPool1d`... : CNN向け\n",
    "- `RNN`, `LSTM`, `GRU`.... : RNN向け\n",
    "- `BatchNorm1d`,`BachNorm2d`, `InstanceNorm1d`,... : バッチ正規化\n",
    "- `functional.hoge` : 活性化関数(SoftMax, ReLUなど)  \n",
    "- その他、dropout, sparse_embeddings,... \n",
    "\n",
    "詳しくは[pytorch:nn](http://pytorch.org/docs/master/nn.html)\n",
    "\n",
    "\n",
    "すべてPyTorch内の関数で渡してやることができれば、back propagation時の微分演算などがスムーズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square you can only specify a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight\n",
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 損失関数\n",
    "誤差の算出のために、いろいろな関数が用意されている。シンプルなのはMean Squared Error。いろんな誤差を知りたい人は、[機械学習で使う指標総まとめ(教師あり学習編)](http://www.procrasist.com/entry/ml-metrics)へ。\n",
    "\n",
    "- MSELoss\n",
    "- CrossEntropyLoss\n",
    "- NLLLoss(negative log likelihood loss) (1d, 2d)\n",
    "- Poisson NLL Loss\n",
    "- KLDivLoss (Kullback-Leibler divergence)\n",
    "- BCELoss (Binary Cross Entropy)\n",
    "- ...\n",
    "\n",
    "より詳しくは[公式](http://pytorch.org/docs/master/nn.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input)\n",
    "target = Variable(torch.arange(1, 11))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでで、こうなっている\n",
    ">input -> conv2d -> relu -> maxpool2d -> conv2d -> relu -> maxpool2d\n",
    "      -> view -> linear -> relu -> linear -> relu -> linear\n",
    "      -> MSELoss\n",
    "      -> loss\n",
    "     \n",
    "この損失からさかのぼっていく"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad() #とりあえず0で初期化\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重みのアップデート\n",
    "学習には、`torch.optim`を使う\n",
    "\n",
    "### アルゴリズム\n",
    "- SGD, ASGD, Nesterov-SGD\n",
    "- Adadelta, Adagrad,  Adam, SparseAdam, Adamax\n",
    "- LBFGS\n",
    "- RMSProp, Rprop\n",
    "\n",
    "### 学習の仕方\n",
    "`torch.optim.lr_scheduler`で学習率の調整もできる。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "### ネットワークの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x)\n",
    "\n",
    "model = Net()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データのロード\n",
    "- `torchvision`からデータをダウンロード(画像系)\n",
    "- `torch.utils.data.DataLoader`にて、データの操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "import torch.utils.data\n",
    "batch_size = 50\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n",
    "    batch_size=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 実行"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.02)\n",
    "model.train()\n",
    "train_loss = []\n",
    "train_accu = []\n",
    "i = 0\n",
    "for epoch in range(10):\n",
    "    for data, target in train_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()    # calc gradients\n",
    "        train_loss.append(loss.data[0])\n",
    "        optimizer.step()   # update gradients\n",
    "        prediction = output.data.max(1)[1]   # first column has actual prob.\n",
    "        accuracy = prediction.eq(target.data).sum()/batch_size*100\n",
    "        train_accu.append(accuracy)\n",
    "        if i % 1000 == 0:\n",
    "            print('Train Step: {}\\tLoss: {:.3f}\\tAccuracy: {:.3f}'.format(i, loss.data[0], accuracy))\n",
    "        i += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
